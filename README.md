# Estudo do Perceptron e aplicações de SLP e MLP em Python

## Resumo

O presente trabalho buscou realizar um estudo do Perceptron e desenvolver
uma implementação em Python do mesmo. A aplicação desenvolvida apresenta duas
partes: i) uma sobre o perceptron de uma única camada (SLP) e ii) a outra sobre o
perceptron de múltiplas camadas (MLP). Na primeira parte, o objetivo foi treinar o SLP a
realizar a soma de dois números reais passados a ele. Na segunda parte, o objetivo foi
treinar o MLP a calcular a tangente hiperbólica de um valor real passado a ele. Para isso,
em ambos os casos (SLP e MLP), utilizou-se o método do gradiente descendente para
minimizar o erro total gerado pelo perceptron. No caso do SLP, o erro convergiu para o
seu valor mínimo. No caso do MLP, o erro também convergiu para seu valor mínimo, no
entanto de forma mais lenta do que no caso do SLP. Os algorítmos implementados no
presente trabalho podem ser utilizados, em trabalhos futuros, para ensinar o perceptron a
aproximar um conjunto de pontos no plano ou no espaço, com tendência linear ou não-
linear.

## Trabalho completo em pdf:
[Perceptrons](./perceptrons.pdf)

O trabalho completo em pdf contém uma introdução aos perceptrons bem como a derivação matemática dos algoritmos utilizados. 
  
